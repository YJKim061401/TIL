{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 딥러닝 개요\n",
    "## 딥\n",
    "- 연속된 층으로 표현을 학습한다는 개념\n",
    "- 데이터로부터 모델을 만드는 데 얼마나 많은 층을 사용했는지가 그 모델의 깊이\n",
    "\n",
    "## 층\n",
    "- 신경망의 핵심 구성 요소\n",
    "- 데이터 처리 필터\n",
    "- 데이터가 입력되면 더 유용한 형태로 출력\n",
    "- 즉, 입력된 데이터로부터 주어진 문제에 더 의미있는 표현 추출\n",
    "\n",
    "## 딥러닝 모델\n",
    "- 데이터 정제 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "여러 개의 층으로 이루어진 네트워크가 이미지 안의 숫자를 인식하기 위해 이미지를 변환하는 예\n",
    "- 최종 출력에 대해 점점 더 많은 정보를 가지지만 원본 이미지와는 점점 더 다른 표현으로 숫자 이미지가 변환됨\n",
    "- 심층 신경망을 정보가 연속된 필터(층)를 통과하면서 순도 높게 정제되는 다단계 정보 추출 작업으로 생각할 수 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 딥러닝\n",
    "- 기술적으로는 데이터 표현을 학습하기 위한 다단계 처리 방식을 말함\n",
    "# 심층 신경망\n",
    "- 입력-타겟 매핑을 간단한 데이터 변환기 (층)를 많이 연결하여 수행\n",
    "- 데이터 변환은 샘플에 노출됨으로써 학습이 이루어짐"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 퍼셉트론 (Perceptron)\n",
    "- 인공 신경망의 한 종류\n",
    "- 다수의 신호(입력)을 입력받아서 하나의 신호 (output)를 출력\n",
    "- 뉴런이 전기신호를 내보내 정보를 전달하는 것과 유사 \n",
    "- 뉴런의 수상돌기나 축색돌기처럼 신호를 전달하는 역할을 퍼셉트론에서는 가중치(weight)가 담당\n",
    "- 가중치가 각각의 입력 신호에 대해 부여되어 입력신호와 계산을 하고\n",
    "- 신호의 총합이 정해진 임계값을 넘으면 다음 뉴런으로 신호를 전달하고 ('뉴런의 활성화') ==> 활성함수\n",
    "- 아니면 아무것도 수행하지 않음\n",
    "\n",
    "- 각 노드에 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단되고 (e.g. 활성함수 중 하나가 시그모이드 함수/ 0이다 1이다 판단)\n",
    "- 입력값과 활성화 함수를 사용해 출력값을 다음으로 넘기는 가장 작은 신경망 단위를 퍼셉트론이라고 함\n",
    "- 가중치는 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수이다. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 가중치, 가중합, Bias, 활성화 함수\n",
    "* 기울기나 절편을 퍼셉트론의 개념에 맞게 설명\n",
    "    * 기울기는 퍼셉트론에서는 **가중치**를 의미하는 w (weight)\n",
    "    * 절편은 **b** (bias)\n",
    "* y = ax+b\n",
    "    * a는 기울기 / b는 절편\n",
    "* y = wx+b\n",
    "    * w는 가중치 / b는 bias\n",
    "\n",
    "* 가중합\n",
    "    * 입력값(x)와 가중치(w)의 곱을 모두 더한 다음 b(bias)를 더한 값\n",
    "\n",
    "* 활성화함수 (Activate function)\n",
    "    * 가중합의 결과를 보고 1 또는 0을 출력해서 (분류일 경우) 다음으로 보냄\n",
    "    * 여기서 0과 1을 판단하는 함수 (e.g. 시그모이드함수)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 퍼셉트론의 한계와 이를 해결하는 과정\n",
    "* 퍼셉트론의 과제 \n",
    "* 직선하나를 그어서 검은점과 흰점을 구분할 수 없음 (책 그림 참고)\n",
    "    * 이 문제는 고정관념을 깨서 기발한 아이디어에서 해결점을 찾았음\n",
    "    * 평면을 휘어주는 것\n",
    "    * 여기서 나온 개념이 **다층 퍼셉트론**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 다층퍼셉트론\n",
    "* 좌표 평면 자체에 변화를 주는 것\n",
    "* 이를 가능하게 하려면 숨어 있는 층, 즉 은닉층(hidden layer)을 만들면 됨\n",
    "* 은닉층이 좌표 평면을 왜곡시키는 결과를 가져옴\n",
    "* 은닉층을 만들어 왜곡하면 두 영역을 가로지르는 선이 직선으로 바뀜 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 다층 퍼셉트론의 설계\n",
    "* 다층 퍼셉트론이 입력층과 출력층 사이에 숨어있는 은닉층을 만드는 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "입력층 -> 은닉층 -> 출력 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 오차 역전파 (Back Propagation)\n",
    "- 단일 퍼셉트론에서 결과값을 얻으면 오차를 구해서 이를 토대로 앞 단계에서 정한 가중치를 조정함\n",
    "- 실제값과 비교해서 가중치 수정\n",
    "- 다중 퍼셉트론을 학습시키는 데 성공하면서 XOR 문제를 해결할 수 있었고\n",
    "- 다층 퍼셉트론은 다층 퍼셉트론을 결합하여 만든 것으로 신경망이라는 개념이 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 다층 퍼셉트론\n",
    "- 결과값의 오차를 구해 이를 토대로 앞선 가중치를 차례로 거슬러 올라가며 조정함\n",
    "- 가중치 업데이트 (출력층과 은닉층의 가중치를 수정함)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 다층 확장 - 오차 역전파\n",
    "- 출력층으로 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 활성화 함수\n",
    "* 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값을 판단하는 함수\n",
    "* 시그모이드가 대표적\n",
    "* 시그모이드함수에서 기울기 소실 문제가 발생\n",
    "* 여러 층을 거칠 수록 기울기가 사라져서 (미분을 통해 기울기를 구함) 가중치를 수정하기 어려운 시그모이드 함수의 특성 때문에 \n",
    "* 이를 해결하고자 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기 시작 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 렐루 함수 (ReLu) 함수\n",
    "* 시그모이드 함수의 대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수\n",
    "* 여러 은닉층을 거치면서 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음\n",
    "* 이 간단한 방법으로 여러 층을 쌓을 수 있고 이로써 딥러닝의 발전에 속도가 붙게 됨\n",
    "* 0미만의 값은 모두 0으로 처리하고 0이상의 값은 원 값을 그대로 출력 \n",
    "\n",
    "## 소프트플러스 - 렐루의 0이 되는 순간을 완화"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 속도와 정확도 문제를 해결하는 고급 경사 하강법\n",
    "## 경사 하강법\n",
    "* 오차가 최소인 점을 찾아서 조금씩 이동\n",
    "* 한 번 업데이트 할때마다 전체 데이터를 미분해서 계산량이 많다는 단점\n",
    "* 속도를 느리게 할 뿐 아니라 최적화를 찾기 전에 최적화 과정이 멈출 수도 있음\n",
    "\n",
    "## 확률 경사 하강법 (고급 경사 하강법)\n",
    "* 경사 하강법의 단점 보완\n",
    "* 전체 데이터를 사용하는 것이 아니라 랜덤하게 추출한 일부 데이터를 사용\n",
    "* 더 빨리 자주 업데이트 가능\n",
    "\n",
    "## 모멘텀 (Momentum) --> 고급 경사 하강법\n",
    "* 경사 하강법에 탄력을 주는 것\n",
    "* 오차를 수정하기 전 바로 앞 수정값과 방향 (+,-)를 참고하여 같은 방향으로 일정한 비율만큼 수정하는 방법\n",
    "* 이전 이동 값을 고려하여 일정한 비율만큼만 다음값을 결정하기 때문에 관성의 효과를 낼 수 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 딥러닝 과정\n",
    "1. 신경은 가중치를 파라미터 가진다\n",
    "* 층에서 입력 데이터 처리되는 상세 내용은 일련의 숫자로 이루어진 층의 가중치 (weight)에 저장되어 있음\n",
    "* 기술적으로 말하면 어떤 층에서 일어나는 변환은 그 층의 가중치를 파라미터로 가지는 함수로 표현\n",
    "* 가중치를 **파라미터**라고도 부른다\n",
    "* 학습은 주어진 입력을 정확하게 매핑하기 위해 신경망의 모든 층에 있는 가중치 값을 찾는 것을 의미\n",
    "\n",
    "* 그렇지만 어떤 심층 신경망은 수천만 개의 파라미터를 가지기도 한다.\n",
    "* 이런 경우에 모든 파라미터의 정확한 값을 찾는 것은 어려움\n",
    "* 파라미터 하나의 값을 바꾸면 다른 모든 파라미터에 영향을 끼침"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 손실함수가 신경망의 출력 품질을 측정\n",
    "* 손실함수(loss-function) 또는 목적 함수\n",
    "* 출력이 기대하는 것보다 얼마나 벗어났는지를 측정해서 신경망의 출력을 제어\n",
    "* 신경망이 한 샘플에 대해서 얼마나 잘 예측했는지를 측정하기 위해\n",
    "* 신경망의 예측값과 실제값(진짜 타겟의 값)의 차이를 계산 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 손실점수를 피드백 신호로 사용하여 가중치를 조정\n",
    "* **옵티마이저(optimizer)** \n",
    "    * 딥러닝의 핵심 알고리즘\n",
    "    * 손실함수가 계산한 손실점수를 피드백 심호로 사용하여 \n",
    "    * 현재 샘플의 손실점수가 감소되는 방향으로 가중치 값을 조금씩 수정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 입력값과 타겟의 매핑\n",
    "* 초기에는 네트워크의 가중치가 랜덤한 값으로 할당되므로 랜덤한 변환을 연속적으로 수행\n",
    "* 자연스럽게 출력은 기대한 것과 멀어지고 손실점수가 매우 높아지는데 \n",
    "* 네트워크가 모든 샘플을 처리하면서 가중치가 조금씩 올바른 방향으로 조정되고 손실점수가 감소함\n",
    "* 이를 **반복 훈련**이라고 함\n",
    "* 충분한 횟수만큼 반복하면 (일반적으로 수천개의 샘플에서 수십번 반복하면)\n",
    "* 손실함수를 최소화하는 가중치 값을 산출함\n",
    "* 최소한의 손실을 내는 네트워크가 타겟에 가능한 가장 가까운 출력을 만드는 모델이 됨"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "3ce190ffd99c5fe60b1c57311dac616e81fe8ecfe313fb75441d2908b5e9c286"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}